import torch
from transformers import AutoTokenizer, TrainerCallback
from trl import AutoModelForCausalLMWithValueHead, DPOTrainer, DPOConfig
from huggingface_hub import login

from openai import OpenAI
from difflib import SequenceMatcher

from datasets import Dataset
import pandas as pd

from torch.utils.tensorboard import SummaryWriter

import os
import json

device = "cuda" if torch.cuda.is_available() else "cpu"

def init():
    """
    Initialize environment settings including GPU selection and authentication for OpenAI and
    Huggingface Hub.
    """
    os.environ["CUDA_VISIBLE_DEVICES"] = "4"  # Use a single specified GPU

    global openai_client 
    openai_client = OpenAI(
        api_key="OPENAI_API_KEY"  # Replace with your OpenAI API key
    )

    login(token="HUGGINGFACE_TOKEN")  # Authenticate with Huggingface Hub

def get_summery_from_model(model: AutoModelForCausalLMWithValueHead,
                           tokenizer: AutoTokenizer,
                           content: str) -> str:
    """
    Generate a concise TL;DR style summary in one sentence from the input content using the model.

    Args:
        model: The language model with value head.
        tokenizer: Corresponding tokenizer.
        content: Text to be summarized.

    Returns:
        A single sentence summary generated by the model.
    """
    message = [
        {
            "role": "user",
            "content": f"Summarize the following text in a TL;DR style \
                in **one sentence**\n\n{content}\n"
        }
    ]
    prompt = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)

    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    attention_mask = torch.ones_like(input_ids)
    
    output_ids = model.generate(
        input_ids,
        attention_mask=attention_mask,
        max_new_tokens=100,
        do_sample=True,
        temperature=0.7,
        top_p=0.95,
        top_k=50,
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id
    )  
    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    # Extract text after the assistant tag
    assistant_tag = "<|assistant|>"
    if assistant_tag in output_text:
        output_text = output_text.split(assistant_tag, 1)[1].strip()

    return output_text

def prepare_raw_data(model: AutoModelForCausalLMWithValueHead,
                     tokenizer: AutoTokenizer, 
                     dataset: Dataset) -> list:
    """
    Generate raw summarization data by querying the model twice for each prompt to produce paired
    responses.

    Args:
        model: The language model for generation.
        tokenizer: Corresponding tokenizer.
        dataset: Dataset containing prompts and reference answers.

    Returns:
        List of dictionaries containing prompt, two model responses, and reference answer.
    """
    print("Preparing raw data...")
    
    raw_data = []

    model.eval()
    with torch.no_grad():
        for data in dataset:
            prompt = data['content']
            answer = data['answer']
            response_a = get_summery_from_model(model, tokenizer, prompt)
            response_b = get_summery_from_model(model, tokenizer, prompt)
            raw_data.append({
                "prompt": prompt,
                "response_a": response_a,
                "response_b": response_b,
                "answer": answer
            })
            print(f"Processed: {len(raw_data)}/{len(dataset)}", end="\r")

    print("\nRaw data preparation complete.")
    return raw_data

def get_preference_from_gpt(prompt: str, 
                            response_a: str, 
                            response_b: str) -> str:
    """
    Simulate human preference between two responses by querying GPT-4o-mini.

    Args:
        prompt: Original prompt text.
        response_a: First model response.
        response_b: Second model response.

    Returns:
        'A' or 'B' indicating which response is preferred.
    """
    user_prompt = f"""
        [PROMPT]
        {prompt}

        [RESPONSE A]
        {response_a}

        [RESPONSE B]
        {response_b}

        Only output a single letter: A or B. 
        Do not explain your answer. 
        Do not include anything else.
        Which response is better?
    """

    completion = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0
    )
    content = completion.choices[0].message.content.strip().upper()
    return "A" if "A" in content else "B"

def get_preference_by_similarity(response_a: str, 
                                 response_b: str,
                                 answer: str) -> str:
    """
    Simulate human preference by comparing similarity of responses to the reference answer.

    Args:
        response_a: First model response.
        response_b: Second model response.
        answer: Reference answer text.

    Returns:
        'A' or 'B' indicating which response is more similar to the answer.
    """
    def similarity(a: str, b: str) -> float:
        return SequenceMatcher(None, a, b).ratio()

    sim_a = similarity(response_a, answer)
    sim_b = similarity(response_b, answer)

    return "A" if sim_a >= sim_b else "B"
    
def prepare_pairwise_data(raw_data: list) -> list:
    """
    Convert raw data into pairwise comparison format with chosen and rejected responses based on
    simulated preference.

    Args:
        raw_data: List of raw data dictionaries containing prompt and paired responses.

    Returns:
        List of dictionaries with prompt, chosen response, and rejected response.
    """
    print("Preparing pairwise compared data...")
    
    pairwise_data = []
    for item in raw_data:
        prompt = item['prompt']
        response_a = item['response_a']
        response_b = item['response_b']
        
        # Use similarity heuristic for preference; can be replaced by GPT-4o-mini preference
        # Determine preference using the reference answer
        answer = item['answer']
        better = get_preference_by_similarity(response_a, response_b, answer)
        if better == "A":
            chosen, rejected = response_a, response_b
        else:
            chosen, rejected = response_b, response_a
        
        pairwise_data.append({
            "prompt": prompt,
            "chosen": chosen,
            "rejected": rejected
        })
        print(f"Processed: {len(pairwise_data)}/{len(raw_data)}", end="\r")

    print("\nPairwise data preparation complete.")
    return pairwise_data

def compute_win_rate(model: AutoModelForCausalLMWithValueHead,
                     tokenizer: AutoTokenizer,
                     pairwise_data: list) -> float:
    """
    Evaluate model performance by computing the win rate over pairwise comparisons using log-likelihood.

    Args:
        model: The language model to evaluate.
        tokenizer: Corresponding tokenizer.
        pairwise_data: List of pairwise comparison data.

    Returns:
        Win rate as a float between 0 and 1.
    """
    model.eval()
    wins = 0
    total = len(pairwise_data)
    
    with torch.no_grad():
        for item in pairwise_data:
            prompt = item["prompt"]
            chosen = item["chosen"]
            rejected = item["rejected"]
            
            def get_logprob(text):
                inputs = tokenizer(prompt + text, return_tensors="pt").to(device)
                outputs = model(**inputs, labels=inputs["input_ids"])
                # Use the loss returned by the model as the negative log-likelihood
                return -outputs.loss.item()
            
            lp_chosen = get_logprob(chosen)
            lp_rejected = get_logprob(rejected)
            
            if lp_chosen > lp_rejected:
                wins += 1
        
    return wins / total

if __name__ == "__main__":
    init()

    # Load model and tokenizer for summarization and DPO training
    print("loading model")
    model_path = "models/sft/TinyLlama/TinyLlama-1.1B-Chat-v1.0"

    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLMWithValueHead.from_pretrained(model_path).to(device)
    model.warnings_issued = {}
    print("model loaded")

    # Load reference model for evaluation and freeze parameters
    print("loading reference model")
    ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_path).to("cpu")
    ref_model.eval()
    for param in ref_model.parameters():
        param.requires_grad = False
    print("reference model loaded")

    # Load dataset with prompts and reference answers
    print("loading dataet")
    df = pd.read_csv("~/.kaggle/cnn_dailymail/train.csv", nrows=3000)  # Load subset of dataset
    df = df[["article", "highlights"]].rename(columns={
        "article": "content",
        "highlights": "answer"
    })
    dataset = Dataset.from_pandas(df)
    print("dataset loaded")

    # Generate raw data by querying model responses
    raw_data = prepare_raw_data(model, tokenizer, dataset)
    with open("cache/raw_data.json", "w", encoding="utf-8") as f:
        json.dump(raw_data, f, ensure_ascii=False, indent=4)

    # Prepare pairwise comparison data from raw responses
    dpo_data = prepare_pairwise_data(raw_data)

    # Convert pairwise data into Dataset for DPO training
    train_dataset = Dataset.from_list(dpo_data)

    # Configure DPO training parameters
    training_args = DPOConfig(
        output_dir="models/rlhf/DPO_pairwise/TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        logging_steps=5,
        logging_dir="logs/DPO_pairwise/TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,
        fp16=True,
        num_train_epochs=5.0,
        save_steps=50,
        save_strategy="steps",
        save_total_limit=10
    )
    trainer = DPOTrainer(
        model=model,
        args=training_args, 
        processing_class=tokenizer, 
        train_dataset=train_dataset,
    )

    # Define callback to compute and log win rate during training
    class WinRateCallback(TrainerCallback):
        def __init__(self, model, tokenizer, eval_data, writer):
            self.model = model
            self.tokenizer = tokenizer
            self.eval_data = eval_data
            self.writer = writer
        
        def on_log(self, args, state, control, logs=None, **kwargs):
            win_rate = compute_win_rate(self.model, self.tokenizer, self.eval_data)
            print(f"\nStep {state.global_step}: Win rate = {win_rate:.4f}\n")
            self.writer.add_scalar("eval/win_rate", win_rate, state.global_step)
            self.writer.flush()

    writer = SummaryWriter(log_dir="logs/DPO_pairwise/TinyLlama/TinyLlama-1.1B-Chat-v1.0")
    trainer.add_callback(WinRateCallback(model, tokenizer, dpo_data, writer))

    # Start DPO training
    trainer.train()