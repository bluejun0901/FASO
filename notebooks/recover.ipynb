{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a70dced6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "PROJECT_ROOT = Path(os.getenv(\"PROJECT_ROOT\")).resolve() # type: ignore\n",
    "MODEL_ROOT = Path(os.getenv(\"MODEL_ROOT\")).resolve() # type: ignore\n",
    "DATA_ROOT = Path(os.getenv(\"DATA_ROOT\")).resolve() # type: ignore\n",
    "CONFIG_ROOT = Path(os.getenv(\"CONFIG_ROOT\")).resolve() # type: ignore\n",
    "SRC_ROOT = Path(os.getenv(\"SRC_ROOT\")).resolve() # type: ignore\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "sys.path.append(str(SRC_ROOT))\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from utils.utility import *\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d7f94b4-056a-40d0-a8f6-d6755bc40ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_68c04c5b38b481999bedc167f30682d50d4f30cbc5d453b3', created_at=1757432923.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_68c04c5be7408199bdf4a8263f6b74120d4f30cbc5d453b3', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_68c04c5c0dfc8199a845fd21ae8c93480d4f30cbc5d453b3', content=[ResponseOutputText(annotations=[], text='Hello! How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=128, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=7, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=15, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=22), user=None, store=True)\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=\"hello\",\n",
    "    reasoning={\n",
    "        \"effort\": \"minimal\"\n",
    "    },\n",
    "    max_output_tokens=128\n",
    ")\n",
    "print(response.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d70b0f7b-ae82-4dd9-9dab-e615eec53fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "print(response.output[1].content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88249ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_68be4c7a56ac819088db9fbf44777491', completion_window='24h', created_at=1757301882, endpoint='/v1/responses', input_file_id='file-8uRxJhZJhPmpcgMpv9sYrd', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-5-mini in organization org-4zGMPv7mHWxPevL6TmmXK8o8. Limit: 40,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1757388282, failed_at=1757301884, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0), usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch(id='batch_68be4c61d0f88190bf95704fee75b326', completion_window='24h', created_at=1757301857, endpoint='/v1/responses', input_file_id='file-BVxeLv8zXxYqL2tKxD1fyA', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1757388257, failed_at=None, finalizing_at=None, in_progress_at=1757301860, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=1990, failed=0, total=2000), usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch(id='batch_68be4c4a9cf081908e2b9636254866a7', completion_window='24h', created_at=1757301834, endpoint='/v1/responses', input_file_id='file-4Kk3vL9Vp6cuR9niYpfFEG', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-5-mini in organization org-4zGMPv7mHWxPevL6TmmXK8o8. Limit: 40,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1757388234, failed_at=1757301836, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0), usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch(id='batch_68be4c32b0948190875347e12e83af8c', completion_window='24h', created_at=1757301810, endpoint='/v1/responses', input_file_id='file-VwfVHrY5BrKU8wfwY9hnnJ', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-5-mini in organization org-4zGMPv7mHWxPevL6TmmXK8o8. Limit: 40,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1757388210, failed_at=1757301814, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0), usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch(id='batch_68be4a9892548190be7b74ee8e218b10', completion_window='24h', created_at=1757301400, endpoint='/v1/responses', input_file_id='file-4Uqn61fWrgtaFXb29v2A3M', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-5-mini in organization org-4zGMPv7mHWxPevL6TmmXK8o8. Limit: 40,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1757387800, failed_at=1757301402, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0), usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})\n"
     ]
    }
   ],
   "source": [
    "batch_list = client.batches.list(limit=100)\n",
    "print(*batch_list.data[0:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3518a851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_68be4a8163448190ae7be72c6257a560', completion_window='24h', created_at=1757301377, endpoint='/v1/responses', input_file_id='file-VDLkhdmiEWQUV3XvMbtZfF', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1757301796, error_file_id=None, errors=None, expired_at=None, expires_at=1757387777, failed_at=None, finalizing_at=1757301637, in_progress_at=1757301379, metadata=None, output_file_id='file-SXxpw9bFvMJQb1YT6GxGNo', request_counts=BatchRequestCounts(completed=2000, failed=0, total=2000), usage={'input_tokens': 1965288, 'output_tokens': 132373, 'total_tokens': 2097661, 'input_tokens_details': {'cached_tokens': 278912}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch(id='batch_68be486f3508819089af52afc4ba0053', completion_window='24h', created_at=1757300847, endpoint='/v1/responses', input_file_id='file-KHJG7Lr19UH12odpdVVA7S', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1757301289, error_file_id=None, errors=None, expired_at=None, expires_at=1757387247, failed_at=None, finalizing_at=1757301138, in_progress_at=1757300850, metadata=None, output_file_id='file-1U8iq1uQTdnhkXqKzcoj6e', request_counts=BatchRequestCounts(completed=2000, failed=0, total=2000), usage={'input_tokens': 2115537, 'output_tokens': 130499, 'total_tokens': 2246036, 'input_tokens_details': {'cached_tokens': 438784}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch(id='batch_68be422607748190abeb9d0b1eb2398c', completion_window='24h', created_at=1757299238, endpoint='/v1/responses', input_file_id='file-Es6Sc2vD4UzuzahipMYu7K', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1757300007, error_file_id=None, errors=None, expired_at=None, expires_at=1757385638, failed_at=None, finalizing_at=1757299889, in_progress_at=1757299239, metadata=None, output_file_id='file-88mzwkY8DT7M7AzXykiYE4', request_counts=BatchRequestCounts(completed=2000, failed=0, total=2000), usage={'input_tokens': 2167519, 'output_tokens': 133556, 'total_tokens': 2301075, 'input_tokens_details': {'cached_tokens': 487296}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch(id='batch_68be3cf4423c819099957c2c714e4dc5', completion_window='24h', created_at=1757297908, endpoint='/v1/responses', input_file_id='file-YWVTrYQAmqWyr9mwybZzfS', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1757298676, error_file_id=None, errors=None, expired_at=None, expires_at=1757384308, failed_at=None, finalizing_at=1757298497, in_progress_at=1757297975, metadata=None, output_file_id='file-9F7gNf4Q3LJjf869JdiEEV', request_counts=BatchRequestCounts(completed=2000, failed=0, total=2000), usage={'input_tokens': 2021497, 'output_tokens': 131183, 'total_tokens': 2152680, 'input_tokens_details': {'cached_tokens': 406144}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch(id='batch_68be38c6a48c819098c60fd6ae5cddf1', completion_window='24h', created_at=1757296838, endpoint='/v1/responses', input_file_id='file-FV5ZRbyHGrpTGXPsboKx77', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1757297902, error_file_id=None, errors=None, expired_at=None, expires_at=1757383238, failed_at=None, finalizing_at=1757297831, in_progress_at=1757296839, metadata=None, output_file_id='file-WXNE7Q9yAgZWVr1rLGsqfh', request_counts=BatchRequestCounts(completed=2000, failed=0, total=2000), usage={'input_tokens': 2223554, 'output_tokens': 133034, 'total_tokens': 2356588, 'input_tokens_details': {'cached_tokens': 712832}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "batch_list = [batch for batch in batch_list if batch.status == \"completed\"]\n",
    "print(*batch_list[0:5], sep='\\n')\n",
    "print(len(batch_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dbe2815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked 183th batch {\"id\": \"batch_req_68ba78d4d90081909d8f457763cd573e\", \"custom_id\": \"140000 True\", \"response\": {\"statu"
     ]
    }
   ],
   "source": [
    "retrieved_text = []\n",
    "for i, batch in enumerate(batch_list):\n",
    "    if batch.output_file_id is not None:\n",
    "        file = client.files.content(batch.output_file_id)\n",
    "        line = file.text.split('\\n')[0]\n",
    "        if len(line.split('\"custom_id\": \"')[1].split('\"')[0].split(' ')) == 2: \n",
    "            retrieved_text.append(file.text)\n",
    "        else:\n",
    "            break\n",
    "    sys.stdout.write(f\"\\rchecked {i}th batch {line[:100]}\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e052e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_68be4b86ea008190bd278f94a4ad2020\", \"custom_id\": \"288000 False\", \"response\": {\"status_code\": 200, \"request_id\": \"b5f7981f76f8df2fb8ebb9f0eb6de1ef\", \"body\": {\"id\": \"resp_68be4ab0faa081\n"
     ]
    }
   ],
   "source": [
    "recoverable = ''.join(retrieved_text[0:70])\n",
    "print(recoverable[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "065961f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('recovering.jsonl', 'w') as file:\n",
    "    file.write(recoverable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c90d46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('recovering.jsonl', 'r') as file:\n",
    "    recoverable = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1a4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(CONFIG_ROOT / \"experiment1.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35e84d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading generated ouputs from /src/gs25009/LLM_DAG_ALLIGN/dataset/preprocessed/generated_pairwise_openai_2025-08-27_06-03-12.json...\n",
      "Loaded successfully\n"
     ]
    }
   ],
   "source": [
    "gen_filename = \"generated_pairwise_openai_2025-08-27_06-03-12.json\"\n",
    "gen_output_path = DATA_ROOT / config.dataset_output_dir / gen_filename\n",
    "print(f\"Loading generated ouputs from {str(gen_output_path)}...\")\n",
    "with open(gen_output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "dataset = Dataset.from_dict(dataset)\n",
    "print(\"Loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f04c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comparisons(dataset: Dataset) -> list[dict]:\n",
    "    pairs = []\n",
    "    for k, example in enumerate(dataset):\n",
    "        prompt = example['prompt'] # type: ignore\n",
    "        ref = \"\"\n",
    "        summaries = example['summaries'] # type: ignore\n",
    "        \n",
    "        for i, y1 in enumerate(summaries):\n",
    "            for j, y2 in enumerate(summaries):\n",
    "                if i < j:\n",
    "                    pairs.append({\n",
    "                        'prompt': prompt,\n",
    "                        'y1': y1,\n",
    "                        'y2': y2,\n",
    "                        'ref': ref,\n",
    "                        'meta': f\"{k}, {i}, {j}\"\n",
    "                    })\n",
    "    \n",
    "    return pairs\n",
    "comparisons = generate_comparisons(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "528ace02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0, 0, 1', None]\n",
      "['0, 0, 2', None]\n",
      "['0, 0, 3', None]\n",
      "['0, 0, 4', None]\n",
      "['0, 0, 5', None]\n"
     ]
    }
   ],
   "source": [
    "result = [[pair['meta'], None] for pair in comparisons]\n",
    "print(*result[0:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a44329ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "pattern = config.scorer.openai.preference_pattern\n",
    "def _parse_output_line(line: str) -> tuple[int, int | None]:\n",
    "    obj = json.loads(line)\n",
    "    idx = int(obj.get(\"custom_id\").split()[0])  # came from compare_batch_0\n",
    "    body = obj.get(\"response\", {}).get(\"body\", {})\n",
    "    output = body.get(\"output\", [])[1].get(\"content\", \"\")[0]\n",
    "    if not output:\n",
    "        return idx, None\n",
    "    output_text = output.get(\"text\", \"\")\n",
    "    match = re.search(pattern, output_text)\n",
    "    if not match:\n",
    "        return idx, None\n",
    "    # Map '1' -> 0 (first shown), '2' -> 1 (second shown)\n",
    "    judged_idx = 0 if match.group(1) == \"1\" else 1\n",
    "    # Convert back to original y1/y2 indexing using recorded swap info\n",
    "    swapped = obj.get(\"custom_id\").split()[1] == \"True\"\n",
    "    orig_idx = (1 - judged_idx) if swapped else judged_idx\n",
    "    return idx, orig_idx\n",
    "\n",
    "for i, line in enumerate(recoverable.split('\\n')):\n",
    "    if len(line) > 0:\n",
    "        idx, comp = _parse_output_line(line)\n",
    "        result[idx][1] = comp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b7f9291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving result to /src/gs25009/LLM_DAG_ALLIGN/dataset/preprocessed/comparison_cyclic_openai_2025-09-09_09-07-47.jsonl...\n",
      "Saved successfully.\n"
     ]
    }
   ],
   "source": [
    "filename = get_filename(\n",
    "    \"comparison\",\n",
    "    config.builder.type,\n",
    "    config.scorer.type,\n",
    "    suffix=\".jsonl\",\n",
    ")\n",
    "output_path = DATA_ROOT / config.dataset_output_dir / filename\n",
    "\n",
    "print(f\"Saving result to {str(output_path)}...\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(str(output_path), \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair, compare in result:\n",
    "        line = json.dumps({\"id\": pair, \"result\": str(compare)}, ensure_ascii=False)\n",
    "        f.write(line + \"\\n\")\n",
    "print(\"Saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55711c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans. 'In a world where animals have more rights to space and food than humans,' said Charlie Leocha, consumer representative on the committee.\\xa0'It is time that the DOT and FAA take a stand for humane treatment of passengers.' But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking? Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased . Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches . Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane. But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch. While most airlines stick to a pitch of 31 inches or above, some fall below this. While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\\nTL;DR:\\n\", 'y1': 'A U.S. panel set up a 31 inch pitch on planes .\\nA standard United Airlines seat has 30 inches of room .\\nSome airlines place more than others .', 'y2': 'At 31 inches, planes with 30-inch pitches are standard airlines .\\nBut new research tests show smaller space is on the rise .\\nSome airlines are larger and more affordable to take away .\\nTests conducted by the Federal Aviation Administration .', 'ref': '', 'meta': '0, 0, 1'}\n"
     ]
    }
   ],
   "source": [
    "print(comparisons[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8701e96d",
   "metadata": {},
   "source": [
    "python3 preprocess_comparisons.py experiment1.yaml generated_pairwise_openai_2025-08-27_06-03-12.json -o comparison_cyclic_openai_2025-09-09_09-03-27.jsonl\n",
    "352661\n",
    "352069"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM_DAG_ALLIGN)",
   "language": "python",
   "name": "llm_dag_allign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
