{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f4ee09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63a5031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_path = Path(str(os.getenv(\"MODEL_ROOT\"))).resolve() / 'sft' / 'TinyLlama' / 'TinyLlama-1.1B-Chat-v1.0'\n",
    "model_path = Path(str(os.getenv(\"MODEL_ROOT\"))).resolve() / 'finetuned' / 'TinyLlama' / 'TinyLlama-1.1B-Chat-v1.0' / 'checkpoint-189'\n",
    "model_path = str(model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "model.warnings_issued = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6570e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(os.getenv(\"DATA_ROOT\")) / \".kaggle\" / \"cnn_dailymail\"\n",
    "df = pd.read_csv(str(dataset_path / \"test.csv\"))\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7acb9d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experts question if  packed out planes are putting passengers at risk .\n",
      "U.S consumer advisory group says minimum space must be stipulated .\n",
      "Safety tests conducted on planes with more leg room than airlines offer .\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0]['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4962538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "hello? who are you?\n",
      " \n",
      "<|assistant|>\n",
      "Hello! I'm a chatbot that can help you with any questions you may have. To get started, simply type \"hello\" in the chat window and I'll respond with a greeting in return. If you have any questions, feel free to ask. I'm always here to help!\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"hello? who are you?\\n\" \n",
    "    }\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=400)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fece1beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(content, model):\n",
    "    message = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Summarize the following text in a TL;DR style in one sentence\\n\\n{content}\\n\"\n",
    "        }\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=500,\n",
    "        do_sample=False,\n",
    "        # temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )  \n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    assistant_tag = \"<|assistant|>\"\n",
    "    if assistant_tag in output_text:\n",
    "        output_text = output_text.split(assistant_tag, 1)[1].strip()\n",
    "\n",
    "    return output_text, output_ids.shape[-1] - input_ids.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "507ea36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The U.S. Department of Transportation's consumer advisory group set up by the government is concerned about the shrinking space on planes .\n",
      "The group says that the government is happy to set standards for animals flying on planes .\n",
      "But it doesn't stipulate a minimum amount of space for humans .\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "content = dataset[0]['article']\n",
    "print(*(get_summary(content, model)), sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
